{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading packages\n",
    "\n",
    "# utils\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# scraping\n",
    "from urllib.request import urlopen, Request\n",
    "from urllib.error import HTTPError, URLError\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`airlinequality.com` had a blocker for the default `urllib` agent, so this workaround was found in order to correctly scrape the reviews.\n",
    "\n",
    "Source:\n",
    "https://stackoverflow.com/questions/16627227/http-error-403-in-python-3-web-scraping\n",
    "\n",
    "Enable some default error handling in case the site cannot be accessed, and tell us why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sneaky_request(url):\n",
    "    \"\"\"\n",
    "    sneaky_request is a function designed to get around some pages blocking web scraping.\n",
    "    It uses a different User-Agent than the default `python urllib/3.X.X`\n",
    "    \n",
    "    Args:\n",
    "        url (str) : url of the website desired to be scraped\n",
    "    \n",
    "    Return:\n",
    "        open_url (HTTPResponse) : the HTTP response of the input URL\n",
    "    \"\"\"\n",
    "    try:\n",
    "        req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        open_url = urlopen(req)\n",
    "    except HTTPError as error:\n",
    "        print(\"Error code: \", error.code)\n",
    "        print(\"The reason for the exception:\", error.reason)\n",
    "    \n",
    "    return open_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_reviews_url = sneaky_request(\"https://www.airlinequality.com/airline-reviews/germanwings/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(gw_reviews_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check to ensure that this has gone correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gw_reviews_url.geturl())\n",
    "print(\"Status:\",gw_reviews_url.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `BeautifulSoup` to explore and scrape the pages for the relevant info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_reviews = BeautifulSoup(gw_reviews_url.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to traverse all of the pages in order to extract all of the reviews; this means opening each subsequent page and extracting each review.\n",
    "\n",
    "The following `while` loop iterates over each subsequent review page, terminating when there are no further pages to scrape.  The airline review information is stored in `reviews`, to be parsed after extracting all of the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the initial list and condition to keep scraping\n",
    "reviews = []\n",
    "keep_going = True\n",
    "\n",
    "while keep_going:\n",
    "    \n",
    "    # store the customer reviews in a list for later parsing\n",
    "    if len(reviews) == 0:\n",
    "        # if it is the first page, create the list\n",
    "        reviews = gw_reviews.find_all(\"article\", {\"itemprop\" : \"review\"})\n",
    "    else:\n",
    "        # concatenate the next pages reviews\n",
    "        for review in gw_reviews.find_all(\"article\", {\"itemprop\" : \"review\"}):\n",
    "            reviews.append(review)\n",
    "    \n",
    "    # find the next page tag, use it to construct the next page to access\n",
    "    # if it is the last page, end the loop\n",
    "    try:\n",
    "        next_page = gw_reviews.find(\"a\", string = \">>\")[\"href\"]\n",
    "        next_page_url = \"https://www.airlinequality.com\" + next_page\n",
    "    except: \n",
    "        keep_going = False\n",
    "        \n",
    "    # open the next page, but wait 5 seconds to be polite \n",
    "    # and not overload the server\n",
    "    time.sleep(5)\n",
    "    gw_reviews_url = sneaky_request(next_page_url)\n",
    "    gw_reviews = BeautifulSoup(gw_reviews_url.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the reviews are all extracted, construct a `pandas` dataframe with desired information.\n",
    "\n",
    "First, double check that all `146` reviews are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through the reviews, building lists of the required information.\n",
    "\n",
    "Note that this could be done in parallel using a library such as [`joblib`](https://joblib.readthedocs.io/en/latest/), but the dataset is so small that there is no need to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some functions to help us later\n",
    "\n",
    "def safe_extract(extracted_tag, replacement_value = None):\n",
    "    \"\"\"\n",
    "    safe_extract is used to extract text from html tags when sometimes the tag doesn't exist.\n",
    "    Instead of throwing an error, it provides a defined replacement value\n",
    "    \n",
    "    Args:\n",
    "        extracted_tag (Tag) : BeautifulSoup html tag containing the desired text\n",
    "        replacement_value (int, bool, str, dbl...) : if the tag doesn't exist, what\n",
    "                                                     should it be replaced with?\n",
    "                                                     \n",
    "    Return:\n",
    "        the extracted text if it exists, if not then the replacement value\n",
    "    \"\"\"\n",
    "    try:\n",
    "        value = extracted_tag.text\n",
    "    except:\n",
    "        value = replacement_value\n",
    "        \n",
    "    return value\n",
    "\n",
    "def sibling_extract(extracted_tag, next_tag = \"td\", replacement_value = None):\n",
    "    \"\"\"\n",
    "    sibling_extract is used to extract text from a html tag's sibling when sometimes the tag doesn't exist.\n",
    "    Instead of throwing an error, it provides a defined replacement value\n",
    "    \n",
    "    Args:\n",
    "        extracted_tag (Tag) : BeautifulSoup html tag containing the desired text\n",
    "        next_tag (str) : next tag type to find\n",
    "        replacement_value (int, bool, str, dbl...) : if the tag doesn't exist, what\n",
    "                                                     should it be replaced with?\n",
    "                                                     \n",
    "    Return:\n",
    "        the extracted text if it exists, if not then the replacement value\n",
    "    \"\"\"\n",
    "    try: \n",
    "        # using find_next to find the sibling with the specified tag\n",
    "        value = extracted_tag.find_next(next_tag).text\n",
    "    except:\n",
    "        value = None\n",
    "    \n",
    "    return value\n",
    "\n",
    "def star_extract(extracted_tag, next_tag = \"td\", replacement_value = None):\n",
    "    \"\"\"\n",
    "    star_extract is used to extract the number of rated stars from a html tag.\n",
    "    When the rating doesn't exist, instead of throwing an error \n",
    "    it provides a defined replacement value\n",
    "    \n",
    "    Args:\n",
    "        extracted_tag (Tag) : BeautifulSoup html tag containing the desired star ratings\n",
    "        next_tag (str) : next tag type to find\n",
    "        replacement_value (int, bool, str, dbl...) : if the tag doesn't exist, what\n",
    "                                                     should it be replaced with?\n",
    "                                                     \n",
    "    Return:\n",
    "        the extracted number of stars if it exists, if not then the replacement value\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # find the number of stars that have class = \"star fill\" representing\n",
    "        # the number of rated stars.\n",
    "        #\n",
    "        # For example, a 4 star rating will have 4 class = \"star fill\" and\n",
    "        # one class = \"star\"\n",
    "        #\n",
    "        # the sibling tag will need to be found as well since\n",
    "        # the class value is not unique for the number of stars\n",
    "        filled_star_tags = extracted_tag.find_next(next_tag).find_all(\"span\", {\"class\" : \"star fill\"})\n",
    "        \n",
    "        # the number of filled in star tags is the rating\n",
    "        value = len(filled_star_tags)\n",
    "        \n",
    "    except:\n",
    "        value = None\n",
    "        \n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "I used the functions above but in retrospect, should have instead used a single function with a format of:\n",
    "\n",
    "```\n",
    "try:\n",
    "    append tag query\n",
    "except:\n",
    "    append None\n",
    "```\n",
    "\n",
    "But in interest of time I kept moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dictionary structure for easily working with\n",
    "parsed_reviews = {\n",
    "    \"title\" : [],\n",
    "    \"review_value\" : [],\n",
    "    \"n_user_reviews\" : [],\n",
    "    \"reviewer_name\" : [],\n",
    "    \"reviewer_country\" : [],\n",
    "    \"date_of_review\" : [],\n",
    "    \"review_text\" : [],\n",
    "    \"aircraft\" :[],\n",
    "    \"traveller_type\" : [],\n",
    "    \"seat_type\" : [],\n",
    "    \"route\" : [],\n",
    "    \"date_flown\" : [],\n",
    "    \"seat_comfort_rating\" : [],\n",
    "    \"cabin_staff_service_rating\" : [],\n",
    "    \"food_and_beverages_rating\" : [],\n",
    "    \"inflight_entertainment_rating\" : [],\n",
    "    \"ground_service_rating\" : [],\n",
    "    \"value_for_money_rating\" : [],\n",
    "    \"recommendation\" : []\n",
    "}\n",
    "\n",
    "\n",
    "# iterate through all reviews, extracting information from each\n",
    "# and storing in the parsed_reviews dict\n",
    "for review in reviews:\n",
    "\n",
    "    # extract review title\n",
    "    review_title = review.find(\"h2\", {\"class\" : \"text_header\"})\n",
    "    parsed_reviews[\"title\"].append(safe_extract(review_title))\n",
    "\n",
    "    # extract review value out of 10\n",
    "    review_value = review.find(\"span\", {\"itemprop\" : \"ratingValue\"})\n",
    "    \n",
    "    # if there is no value out of 10, enter None instead using `safe_extract`\n",
    "    parsed_reviews[\"review_value\"].append(safe_extract(review_value))\n",
    "\n",
    "    # extract number of reviews by the reviewer\n",
    "    n_reviews = review.find(\"span\", {\"class\" : \"userStatusReviewCount\"})\n",
    "    parsed_reviews[\"n_user_reviews\"].append(safe_extract(n_reviews))\n",
    "    \n",
    "    # extract the reviewer\n",
    "    reviewer_name = review.find(\"span\", {\"itemprop\" : \"name\"})\n",
    "    parsed_reviews[\"reviewer_name\"].append(safe_extract(reviewer_name))\n",
    "\n",
    "    # extract the country of the reviewer\n",
    "    reviewer_country = review.find(\"h3\", {\"class\" : \"text_sub_header userStatusWrapper\"})\n",
    "    parsed_reviews[\"reviewer_country\"].append(safe_extract(reviewer_country))\n",
    "    \n",
    "    # extract the date of the review\n",
    "    date_of_review = review.find(\"time\", {\"itemprop\" : \"datePublished\"})\n",
    "    parsed_reviews[\"date_of_review\"].append(safe_extract(date_of_review))\n",
    "    \n",
    "    # extract the review text\n",
    "    review_text = review.find(\"div\", {\"class\" : \"text_content\"})\n",
    "    parsed_reviews[\"review_text\"].append(safe_extract(review_text))\n",
    "    \n",
    "    # extract the aircraft\n",
    "    # there are multiple td with class = \"review-value\"\n",
    "    # so we need to find the sibling header for aircraft then find it's sibling \n",
    "    # in order to find the aircraft type.  Use sibling_extract for this\n",
    "    aircraft = review.find(\"td\", {\"class\" : \"review-rating-header aircraft\"})\n",
    "    aircraft_value = sibling_extract(aircraft)\n",
    "    parsed_reviews[\"aircraft\"].append(aircraft_value)\n",
    "    \n",
    "    # extract the type of traveller\n",
    "    traveller_type = review.find(\"td\", {\"class\" : \"review-rating-header type_of_traveller\"})\n",
    "    traveller_type_value = sibling_extract(traveller_type)\n",
    "    parsed_reviews[\"traveller_type\"].append(traveller_type_value)\n",
    "    \n",
    "    # extract seat type\n",
    "    seat_type = review.find(\"td\", {\"class\" : \"review-rating-header cabin_flown\"})\n",
    "    seat_type_value = sibling_extract(seat_type)\n",
    "    parsed_reviews[\"seat_type\"].append(seat_type_value)\n",
    "    \n",
    "    # extract the route\n",
    "    route = review.find(\"td\", {\"class\" : \"review-rating-header route\"})\n",
    "    route_value = sibling_extract(route)\n",
    "    parsed_reviews[\"route\"].append(route_value) \n",
    "\n",
    "    # extract the date flown\n",
    "    date_flown = review.find(\"td\", {\"class\" : \"review-rating-header date_flown\"})\n",
    "    date_flown_value = sibling_extract(date_flown)\n",
    "    parsed_reviews[\"date_flown\"].append(date_flown_value) \n",
    "\n",
    "    # extract the seat comfort rating out of 5\n",
    "    # need to find the sibling in order to narrow down the number of stars for \n",
    "    # seat comfort or other ratings\n",
    "    seat_comfort_rating = review.find(\"td\", {\"class\" : \"review-rating-header seat_comfort\"})\n",
    "    parsed_reviews[\"seat_comfort_rating\"].append(star_extract(seat_comfort_rating))\n",
    "    \n",
    "    # extract the cabin staff service rating out of 5\n",
    "    cabin_staff_service_rating = review.find(\"td\", {\"class\" : \"review-rating-header cabin_staff_service\"})\n",
    "    parsed_reviews[\"cabin_staff_service_rating\"].append(star_extract(cabin_staff_service_rating))\n",
    "\n",
    "    # extract the food and beverages rating out of 5\n",
    "    food_and_beverages_rating = review.find(\"td\", {\"class\" : \"review-rating-header food_and_beverages\"})\n",
    "    parsed_reviews[\"food_and_beverages_rating\"].append(star_extract(food_and_beverages_rating))\n",
    "    \n",
    "    # extract the inflight entertainment rating out of 5\n",
    "    inflight_entertainment_rating = review.find(\"td\", {\"class\" : \"review-rating-header inflight_entertainment\"})\n",
    "    parsed_reviews[\"inflight_entertainment_rating\"].append(star_extract(inflight_entertainment_rating))\n",
    "    \n",
    "    # extract the ground service rating out of 5\n",
    "    ground_service_rating = review.find(\"td\", {\"class\" : \"review-rating-header ground_service\"})\n",
    "    parsed_reviews[\"ground_service_rating\"].append(star_extract(ground_service_rating))\n",
    "\n",
    "    # extract the value for money rating out of 5\n",
    "    value_for_money_rating = review.find(\"td\", {\"class\" : \"review-rating-header value_for_money\"})\n",
    "    parsed_reviews[\"value_for_money_rating\"].append(star_extract(value_for_money_rating))\n",
    "\n",
    "    # extract if the review recommended Germanwings or not\n",
    "    recommendation = review.find(\"td\", {\"class\" : \"review-rating-header recommended\"}).find_next(\"td\")\n",
    "    parsed_reviews[\"recommendation\"].append(recommendation.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all information is parsed, convert to a `pandas` dataframe and save as a `csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_reviews_df = pd.DataFrame(parsed_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There remains some cleanup to do, but we will save this intermediate dataset and clean it in the next notebook/script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
